{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional functions and plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CelebA,MNIST\n",
    "import torchvision.datasets as dset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from AE import *\n",
    "from Sampling import *\n",
    "from Metric import *\n",
    "import warnings\n",
    "import timeit\n",
    "warnings.filterwarnings(\"ignore\",category=UserWarning)\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "import rpy2.robjects.numpy2ri\n",
    "import rpy2.robjects as robjects\n",
    "from rpy2.robjects.packages import importr\n",
    "robjects.numpy2ri.activate()\n",
    "base = importr('base')\n",
    "rvinecop = importr('rvinecopulib')\n",
    "\n",
    "trans0 = transforms.Compose([\n",
    "    transforms.CenterCrop(140),\n",
    "    transforms.Scale((64,64)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "# Load data\n",
    "path = %pwd\n",
    "dataset_train = CelebA(path,split=\"train\", transform=trans0, download=False)\n",
    "dataset_test = CelebA(path,split=\"test\", transform=trans0, download=False)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=2000, shuffle=True)\n",
    "img_test,attr = next(iter(dataloader_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AE \n",
    "model_AE = AE_Celeba()\n",
    "model_AE.load_state_dict(torch.load('./ae_celebA_200.pth',map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE \n",
    "model_VAE = VAE_Celeba(image_size=64, channel_num=3, kernel_num=128, z_size=100)\n",
    "model_VAE.load_state_dict(torch.load('./vae_CelebA_200.pth'\n",
    "                                          ,map_location=torch.device('cpu')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=2000\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=n, shuffle=True)\n",
    "img,attr = next(iter(dataloader_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get latent variable\n",
    "with torch.no_grad():\n",
    "    lv = model_AE.encode(img)\n",
    "lv = lv.detach().numpy()\n",
    "img_new_AE = model_AE.decode(torch.tensor(lv).float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear interpolation in latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate(img):\n",
    "    n1 = 100; n2 = 10 #n1=100\n",
    "    output = torch.ones((n1,n2))\n",
    "    for i in range(n1):\n",
    "        inter = torch.linspace(img[0,i],img[1,i],n2)\n",
    "        output[i] = inter\n",
    "    return output.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples to interpolate on\n",
    "lv_interpo = lv[[66,1,2,7,8,155,775,17,67,16],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 9))\n",
    "plt.subplots_adjust(wspace =0, hspace =0)\n",
    "for i in range(5):\n",
    "    output = interpolate(lv_interpo[2*i:2*i+2,:])\n",
    "    img_new = model_AE.decode(torch.tensor(output).float())\n",
    "    for j in range(10):\n",
    "        ax = plt.subplot(5, 10, i*10+j+1)\n",
    "        with torch.no_grad():\n",
    "            plt.imshow(img_new[j].reshape((3,64,64)).permute(1,2,0)) \n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combination of marginals and copula of different classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Classes\n",
    "21: male, 16: glasses, 9:black hair, 10:blond hair, 12:brown hair, 18:grey hair, 32:smiling, 23:mustasch\n",
    "36: wearingHat, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose input\n",
    "l1 = []; l2 = []; attr_nr1 =32; attr_nr2 =32\n",
    "for i,label in enumerate(attr):\n",
    "    if label[11-1] == 0: #not blurry\n",
    "        if label[attr_nr1-1] == 0: #\n",
    "            l1.append(i)\n",
    "        if label[attr_nr1-1] == 1: #\n",
    "            l2.append(i)\n",
    "x = lv[l1,:] # copula\n",
    "y = lv[l2,:] # margins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate new images with model\n",
    "n_sample = 10\n",
    "samples_manip = sampling1(x, y, n_sample, seed=123)\n",
    "img_new_manip = model_AE.decode(torch.tensor(samples_manip).float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the closest neighbour\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_neighbour(lv_samples,n_sample):\n",
    "    # return lv of nearest neighbor in latent space of original data\n",
    "    lv_xy = torch.tensor(lv) \n",
    "    dist = distance(lv_samples,lv_xy,\"cpu\")\n",
    "    val, idx = dist.topk(1, 1, False)\n",
    "    lv_neighbor = lv_xy[idx[:,0],:]\n",
    "    index =idx[]\n",
    "    return  lv_neighbor, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New samples\n",
    "n_sample=4\n",
    "lv_samples_EBCAE = sampleing1(lv, lv, n_sample, seed=123)\n",
    "lv_neighbor_EBCAE, index = find_neighbour(lv_samples_EBCAE,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode\n",
    "img_new_EBCAE = model_AE.decode(lv_samples_EBCAE)\n",
    "img_neighbor_AE_EBCAE = model_AE.decode(lv_neighbor_EBCAE)\n",
    "img_neighbor_EBCAE = img[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.subplots_adjust(wspace =0, hspace =0)\n",
    "for i in range(12):\n",
    "    ax = plt.subplot(3, 4, i + 1)\n",
    "    with torch.no_grad():\n",
    "        if i < 4: \n",
    "            plt.imshow(img_new_EBCAE[i].reshape((3,64,64)).permute(1,2,0)) \n",
    "        elif i <8:\n",
    "              plt.imshow(img_neighbor_AE_EBCAE[i-4].reshape((3,64,64)).permute(1,2,0)) \n",
    "        else: \n",
    "            plt.imshow(img_neighbor_EBCAE[i-8].reshape((3,64,64)).permute(1,2,0))            \n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
